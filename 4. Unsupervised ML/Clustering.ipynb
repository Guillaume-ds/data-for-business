{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>body {text-align: justify}</style>\n",
    "\n",
    "Unsupervised machine learning consist in working with unlabeled data in order to create clusters or groups of observations sharing similar features. Contrary to supervised learning, the dataset only stores observations :\n",
    "\n",
    "$$\n",
    "\n",
    "\\mathcal{D} = \\{X_1, ... ,X_n \\} \\in \\mathcal{X}^n\n",
    "\n",
    "$$\n",
    "\n",
    "However, the dimension of $ \\mathcal{X} $ often makes it hard to work with. Therefore, one the first task to do in unsupervised machine learning is to reduce the dimension of the space. This can be done with a map $ \\phi $ from $ \\mathcal{X} $ to a new space $ \\mathcal{X}' $ of smaller dimension. It is important in this process to take a close look at the reconstruction error of the application $ \\tilde{\\phi} $ from $ \\mathcal{X}' $ to $ \\mathcal{X} $, and at the relationship preservation : $(\\phi(X_i),\\phi(X_j))$ should have a similar relationship as $ (X_i,X_j)$\n",
    "\n",
    "The hight dimensional geometry curse - Folks theorem : \n",
    "\n",
    "- If $X_1,...,X_n$ in the hypercube of deimension d such that their coordinates are i.i.d then : \n",
    "\n",
    "$$\n",
    "\\frac{min \\vert\\vert X_i - X_j \\vert\\vert_p}{max \\vert\\vert X_i - X_j \\vert\\vert_p} = 1 + \\mathcal{O}_p(\\sqrt{\\frac{log(n)}{d}})\n",
    "$$\n",
    "\n",
    "That means that when d is large enough, all points are almost equidistant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "As the name suggests, the idea behind principal component analysis is to select the most important features in order to create a subspace of smaller dimension that retains the most information, it aims at decreasing the dimension of the dataset from $n$ to $d$ with $ n \\ge d$. We consider the dispertion of the data with respect to a feature as the importance of this feature, i.e the more dispersed the data is with respect to a feature, the more useful this feature will be in order to create clusters.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "<img src=\"./pictures/PCA_rotation.png\">\n",
    "<figcaption>Fig.1 - PCA example from wikipedia</figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "On the example above, we can clearly see that the feature represented on the X-axis is the principal component, whereas the one on the Y-axis is less important. Indeed, the variation is greater on the X-axis as it is on the Y-axis. The objective will be to rotate the data as shown in red in order to increase the variance with respect to the X axis, and decrease the variance with respect to the Y axis. \n",
    "\n",
    "In a real life scenario where there are many more features and thus dimensions, when classify the importance of each component by computing the covariance matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "There are a variety of clustering methods, which are very difficult to compare. The image below from scikit learn documentation shows \n",
    "<center>\n",
    "<figure>\n",
    "<img src=\"./pictures/plot_cluster_comparison.png\">\n",
    "<figcaption>Fig.1 - Clustering method from scikit learn</figcaption>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
