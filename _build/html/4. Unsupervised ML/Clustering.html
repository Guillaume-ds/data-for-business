
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Dimension Reduction and Clustering &#8212; Data Science for Business</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Introduction to Neural Networks" href="../5.%20Deep%20Learning/1.%20Introduction%20to%20Neural%20Networks.html" />
    <link rel="prev" title="Classification" href="../3.%20Supervised%20ML/Classification.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/datascience.webp" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science for Business</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Data Science for business
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Python and Data Science
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../1.%20Intro/What%20is%20data%20science.html">
   What is Data Science ?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1.%20Intro/What%20is%20python.html">
   What is python ?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1.%20Intro/Presentation%20of%20this%20blog.html">
   Presentation of this Blog
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Manipulation and Data Visualization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2.%20Data%20visualisation/Data%20manipulation.html">
   Data Manipulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2.%20Data%20visualisation/Data%20visualization.html">
   Data Visualization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Supervised Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../3.%20Supervised%20ML/Regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3.%20Supervised%20ML/Classification.html">
   Classification
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unsupervised Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Dimension Reduction and Clustering
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Learning and Optimization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../5.%20Deep%20Learning/1.%20Introduction%20to%20Neural%20Networks.html">
   Introduction to Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5.%20Deep%20Learning/2.%20Deep%20Learning%20Optimization.html">
   Deep Learning Problems and Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5.%20Deep%20Learning/3.%20Convolution%20Neural%20Networks.html">
   Convolutional Neural Networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Science for Finance
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../6.%20Data%20Science%20for%20Finance/Financial%20Data.html">
   Financial Data
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Science and Sustainability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../7.%20Data%20Science%20and%20Sustainability/Sustainable%20Data.html">
   Sustainable Data
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/4. Unsupervised ML/Clustering.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/4. Unsupervised ML/Clustering.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-unsupervised-machine-learning">
   What is unsupervised machine learning ?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimension-reduction">
   Dimension reduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principal-component-analysis">
   Principal Component Analysis
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#first-example-in-2-dimensions">
     First example in 2 dimensions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalisation-in-more-dimensions">
     Generalisation in more dimensions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clustering">
   Clustering
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Dimension Reduction and Clustering</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-unsupervised-machine-learning">
   What is unsupervised machine learning ?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimension-reduction">
   Dimension reduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principal-component-analysis">
   Principal Component Analysis
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#first-example-in-2-dimensions">
     First example in 2 dimensions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalisation-in-more-dimensions">
     Generalisation in more dimensions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clustering">
   Clustering
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="dimension-reduction-and-clustering">
<h1>Dimension Reduction and Clustering<a class="headerlink" href="#dimension-reduction-and-clustering" title="Permalink to this headline">#</a></h1>
<style>body {text-align: justify}</style>
<section id="what-is-unsupervised-machine-learning">
<h2>What is unsupervised machine learning ?<a class="headerlink" href="#what-is-unsupervised-machine-learning" title="Permalink to this headline">#</a></h2>
<p>Unsupervised machine learning consists in working with unlabeled data, often in order to create clusters or groups of observations sharing similar features. For instance, creating a customer segmentation is an unsupervised learning task. It differs from a supervised learning task such as classification in the sense that the segments are not given, i.e. the algorithm has to <em>create</em> the segments in order to classify the customers. Contrary to supervised learning, the dataset is only composed of observations, and does not have target features :</p>
<ul class="simple">
<li><p>Dataset in the case of supervised machine learning : <span class="math notranslate nohighlight">\( \mathcal{D} = \{(X_1,Y_1), ... ,(X_n,Y_n) \} \in \mathcal{X}^n, \mathcal{Y}^n \)</span></p></li>
<li><p>Dataset in the case of unsupervised machine learning : <span class="math notranslate nohighlight">\( \mathcal{D} = \{X_1, ... ,X_n \} \in \mathcal{X}^n \)</span></p></li>
</ul>
<p>Letting the machine create the segments will allow the user to discover hidden patterns and insights that are often quite complex to detect, especially with a large number of features. For example, it can be relatively easy to find similar behaviour for customers that are the same age, but the task gets harder as we increase the number of feature, and comparing the behaviour of customers with respect to their age, location, average purchases, day of the week … can be way harder. Unsupervised machine learning alogrithm aim at finding a solution for such tasks.</p>
</section>
<section id="dimension-reduction">
<h2>Dimension reduction<a class="headerlink" href="#dimension-reduction" title="Permalink to this headline">#</a></h2>
<p>The dimension of <span class="math notranslate nohighlight">\( \mathcal{X} \)</span> often makes it hard to work with. Therefore, one the first task to take care of in unsupervised machine learning is to reduce the dimension of the space under study. This can be done with a map <span class="math notranslate nohighlight">\( \phi \)</span> from <span class="math notranslate nohighlight">\( \mathcal{X} \)</span> to a new space <span class="math notranslate nohighlight">\( \mathcal{X}' \)</span> of smaller dimension, i.e. a function that will transform the elements of <span class="math notranslate nohighlight">\( \mathcal{X} \)</span> into elements of <span class="math notranslate nohighlight">\( \mathcal{X}' \)</span>. It is important in this process to take a close look at the reconstruction error of the application <span class="math notranslate nohighlight">\( \tilde{\phi} \)</span> from <span class="math notranslate nohighlight">\( \mathcal{X}' \)</span> to <span class="math notranslate nohighlight">\( \mathcal{X} \)</span> : <span class="math notranslate nohighlight">\( \tilde{\phi}(\phi(\mathcal{X})) \)</span> should be equal to <span class="math notranslate nohighlight">\( \mathcal{X} \)</span>. The relationship between observations must also be preserved : <span class="math notranslate nohighlight">\((\phi(X_i),\phi(X_j))\)</span> should have a similar relationship as <span class="math notranslate nohighlight">\( (X_i,X_j)\)</span>.</p>
<p>The formalised motivation behind the reduction of the dimension of the space is the <strong>hight dimensional geometry curse - Folks theorem</strong>, which states that <em>when <span class="math notranslate nohighlight">\(d\)</span> is large enough, all points are almost equidistant</em>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(X_1,...,X_n\)</span> in the hypercube of dimension <span class="math notranslate nohighlight">\(d\)</span> such that their coordinates are i.i.d then :</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{min \vert\vert X_i - X_j \vert\vert_p}{max \vert\vert X_i - X_j \vert\vert_p} = 1 + \mathcal{O}_p(\sqrt{\frac{log(n)}{d}})
\]</div>
<p>In reality, creating such a map is difficult and reducing the dimension of the space can be a challenge. For instance, we can consider a dataset about customers with many information that we will resize in order to obtain a smaller dataset by grouping some similar features together :</p>
<center>
<figure>
<img src="./pictures/resized_dataset.png" width="500" height="250">
</figure>
</center>
<p>However, reducing the size of the space leads to a loss of information. Indeed, each dimension corresponds to a feature, thus a space of dimension <span class="math notranslate nohighlight">\(k \le d\)</span> will include <span class="math notranslate nohighlight">\(d-k\)</span> less features.  Therefore, it is necessary to carefully select the dimensions to be removed in ordeer to preserve the principal components.</p>
</section>
<section id="principal-component-analysis">
<h2>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">#</a></h2>
<p>As the name suggests, the objective of principal component analysis is to select the most important features in order to create a subspace of smaller dimension that retains the most information, it aims at decreasing the dimension of the dataset from <span class="math notranslate nohighlight">\(d\)</span> to <span class="math notranslate nohighlight">\(k\)</span> with <span class="math notranslate nohighlight">\( d \ge k\)</span>, while capturing the essence of the original data. We consider the dispertion of the data with respect to a feature as the importance of this feature, i.e the more dispersed the data is with respect to a feature, the more useful this feature will be in order to create clusters.</p>
<section id="first-example-in-2-dimensions">
<h3>First example in 2 dimensions<a class="headerlink" href="#first-example-in-2-dimensions" title="Permalink to this headline">#</a></h3>
<center>
    <figure>
        <img src="./pictures/PCA_rotation.png">
        <figcaption>Fig.1 - PCA example from wikipedia with the transformation to apply</figcaption>
    </figure>
</center>
<p>On the example above, we can clearly see that the feature represented on the X-axis is the principal component, whereas the one on the Y-axis is less important. Indeed, the variation is greater on the X-axis as it is on the Y-axis. The objective will be to rotate the data as shown in red in order to increase the variance with respect to the X axis, and decrease the variance with respect to the Y axis. A little bit of mathematical formalization is necessary in order to clearly understand how one can compute the importance and the dispersion of each feature.</p>
<ol class="simple">
<li><p><strong>Find the center of the dataset</strong>. We can see on the graph above that the origin of the 2 vectors is at the center of the dataset. This point can be found by computing the mean of each feature, here we see that the mean of the data on the X axis is 1, and that the mean on the Y axis is 3. Therefore, the center of the dataset is the point (1,3). Then when move the data set so that its center is on the origin. This operation does not change the dataset, we only “move” the axis so that the origin is on the mean.</p></li>
<li><p><strong>Find the direction of the main vector</strong>. The data above has a nice shape that makes quite obvious the direction in which the main vector should be. However, it is still necessary to compute the precise direction. This can be done by fitting a line through the dataset, the same way than for a linear regression problem, either by minimizing the square distances between the line and the points (<span class="math notranslate nohighlight">\(d_i\)</span>) or by maximizing the square distances between the projection of the point on the line (<span class="math notranslate nohighlight">\(d*_i\)</span>) and the origin. The equivalent holds because of the pytagorian theorem : since : <span class="math notranslate nohighlight">\(b^2_i = d_i^2 + d_i^{*2}\)</span>, then : <span class="math notranslate nohighlight">\(min(\sum_{i=1}^d d_i^2) \equiv max(\sum_{i=1}^d d^{*2}_i)\)</span>. When can have this intuition by looking at the line : as the line gets closer to the data point <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(d_i\)</span> decreases and <span class="math notranslate nohighlight">\(d*_i\)</span> increases as it gets closer to its hypothenuse.</p></li>
</ol>
<center>
    <figure>
        <img src="./pictures/fit_line.png" width="450" height="300">
        <figcaption>Fig.2 - Line fitting through the dataset</figcaption>
    </figure>
</center>
<p>Then, we can find the best fitting line by solving one of the two optimisation problem above. Here, the fitting line as a slope of 1/3, i.e. for each variation of 3 on the X axis, there is only a variation of 1 on the Y axis. This means that the feature X is 3 times more important than Y for the PC 1, and thus the PC1 can be seen as the result of a linear combination such as : 3.X + Y. Indeed, in principal component analysis, we replace the features by a linear combination of the features. Therefore, a data point will not depend on <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, but on two linear combination of both.</p>
<p>Now that we have the the direction of the fitting line, we need to find the unit vector, i.e. the vector that corresponds to moving of one unit on the fitting line. Note that this unit vector corresponds to the eingenvector. We start from the vector above <span class="math notranslate nohighlight">\((3,1)\)</span> that we will scale to a length of 1 by using the pytagorian theorem. Obviously, the lienar relation between X and Y remains, as : <span class="math notranslate nohighlight">\(3 \times \frac{1}{\sqrt{10}} = \frac{3}{\sqrt{10}}\)</span>. This means that for the principal component 1, that we see on the figure below, the feature X is 3 times as important as the feature Y.</p>
<div class="math notranslate nohighlight">
\[
length_{vectorA}^2 = 3^2 + 1^2 \iff length_{vectorA} = \sqrt{10}
\]</div>
<div class="math notranslate nohighlight">
\[
PC1\ unit\ vector = (\frac{3}{\sqrt{10}},\frac{1}{\sqrt{10}})
\]</div>
<center>
    <figure>
        <img src="./pictures/unit_vector.png" width="450" height="300">
        <figcaption>Fig.3 - Computing the unit vector</figcaption>
    </figure>
</center>
<p>Because there are only 2 dimensions in this example, the PC2 is the line perpendicular to PC1 that goes through the origin. Indeed, in order to classify to importance of each PC, they have to be independent, so PC2 has to be orthogonal to PC1, which, in a 2D space, is a perpendicular line. Otherwise, a part of a change in PC1 could be explained as a change in PC2,wich can easely be represented graphically. The slope of the PC2 line is -3, wich means that PC2 is a linear combination of the form : -X+3Y. Then again we scale the vector :</p>
<div class="math notranslate nohighlight">
\[
PC2\ unit\ vector = (\frac{-1}{\sqrt{10}},\frac{3}{\sqrt{10}})
\]</div>
<p>Once we have the two unit vectors and therefore the two PC, we can project the data set onto this new space. Then we rotate the new space and we obtain a new representation of the dataset that extends the most important feature. We can see that the relation between the points remains the same as on fig.2, however the variance increased on the X axis and decreased on the Y axis.</p>
<div style="display:flex;justify-content:space-around">
    <img src="./pictures/PCA_projection.png" width="450" height="300">
    <img src="./pictures/PCA_newspace.png" width="450" height="300" style="transform:rotate(-4deg);">
</div>
<p>We can see that the points are now represented as a function of PC1 and PC2, both of which are linear combinations of X and Y. Finally, we can compute the importance of each feature in the total variation of the PCs. First, we compute the sum of square distances of the projections of the points relatively to the origin divided by (n-1). For PC1 it is the sum of square distances between each green cross and the origin devided by 9 (there are 10 data points), and for PC2 we do the same with the blue crosses. Let’s say for the sake of the example that the variation for PC1 is equal to 15, and that it is equal to 5 for PC2. Then the total variation is equal to 20, and PC1 accounts for 15/20 = 75% of the variation, and PC2 accounts for 25% of the variation.</p>
</section>
<section id="generalisation-in-more-dimensions">
<h3>Generalisation in more dimensions<a class="headerlink" href="#generalisation-in-more-dimensions" title="Permalink to this headline">#</a></h3>
<p>In a real life scenario where there are many more features and dimensions, it is impossible to proceed by representing the dataset in the entire space. Indeed, even the simple example above of a customer dataset is in 5 dimensions, and thus impossible to represent graphically. In order to classify the importance of each component, it is necessary to adopt a mathematical approach and to compute the covariance matrix. The covariance matrix, called <span class="math notranslate nohighlight">\(\Sigma\)</span>, is a matrix where the element <span class="math notranslate nohighlight">\((i,j)\)</span> corresponds to the covariance of the features <span class="math notranslate nohighlight">\(X_i,X_j\)</span>. Note : the covariance of <span class="math notranslate nohighlight">\(X_i,X_i\)</span> is equal to the variance of <span class="math notranslate nohighlight">\(X_i\)</span>, therefore the diagonal of the covariance matrix is composed of the variance of each feature.</p>
<div class="math notranslate nohighlight">
\[
cov(X_i,X_j) = E[(X_i - \mu_i)(X_j - \mu_j)] 
\]</div>
<div style="text-align:center">
<p>with <span class="math notranslate nohighlight">\(\mu_i\)</span>, <span class="math notranslate nohighlight">\(\mu_j\)</span> the expected values of the <span class="math notranslate nohighlight">\(i^{th}\)</span> and <span class="math notranslate nohighlight">\(j^{th}\)</span> features.</p>
</div>
<div class="math notranslate nohighlight">
\[
\Sigma = E[(\boldsymbol{X}−E[\boldsymbol{X}])(\boldsymbol{X}−E[\boldsymbol{X}])^T]
\]</div>
<div style="text-align:center">
<p>with <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> the matrix of observations.</p>
</div>
<p>In order to do a PCA, we start by computing the covariance matrix, and then we compute the corresponding eingenvectors and eigenvalues. Because the features are supposed to be linearly independent, we should find the same number of eingenvectors and eingenvalues as features. We classify the eigenvalues in an increasing order and select the k-biggest (the data scientist gets to choose k!). Finally we project the dataset onto the newly formed space of size k.</p>
<center>
    <figure>
        <img src="./pictures/PCA_summary.png" width="600" height="300">
        <figcaption>Fig.4 - Summary</figcaption>
    </figure>
</center>
<p>In conclusion, we start by looking at the dataset in order to compute the covariance matrix. Then we compute the eigenvectors and eigenvalues. Finally, we can project the data onto a smaller space. Here, the space is in 4 dimensions, which can be represented with the help of a color chart.</p>
</section>
</section>
<section id="clustering">
<h2>Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">#</a></h2>
<p>There are a variety of clustering methods, which are very difficult to compare. The image below from scikit learn documentation shows</p>
<center>
<figure>
<img src="./pictures/plot_cluster_comparison.png">
<figcaption>Fig.1 - Clustering method from scikit learn</figcaption>
</figure>
</center></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./4. Unsupervised ML"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../3.%20Supervised%20ML/Classification.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Classification</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../5.%20Deep%20Learning/1.%20Introduction%20to%20Neural%20Networks.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Introduction to Neural Networks</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Guillaume de Surville<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>