
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Classification &#8212; Data Science for Business</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Unsupervised Machine Learning and Dimension Reduction" href="../4.%20Unsupervised%20ML/1.%20Dimension%20Reduction.html" />
    <link rel="prev" title="Regression" href="Regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/datascience.webp" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science for Business</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Data Science for business
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Python and Data Science
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../1.%20Intro/What%20is%20data%20science.html">
   What is Data Science ?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1.%20Intro/What%20is%20python.html">
   What is python ?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1.%20Intro/Presentation%20of%20this%20blog.html">
   Presentation of this Blog
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Working with Data
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2.%20Data%20visualisation/Data%20manipulation.html">
   Data Manipulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2.%20Data%20visualisation/Data%20visualization.html">
   Data Visualization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Supervised Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Classification
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unsupervised Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4.%20Unsupervised%20ML/1.%20Dimension%20Reduction.html">
   Unsupervised Machine Learning and Dimension Reduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4.%20Unsupervised%20ML/2.%20Principal%20Component%20Analysis.html">
   Principal Component Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4.%20Unsupervised%20ML/3.%20Clustering.html">
   Clustering
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Learning and Optimization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../5.%20Deep%20Learning/1.%20Introduction%20to%20Neural%20Networks.html">
   Introduction to Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5.%20Deep%20Learning/2.%20Deep%20Learning%20Optimization.html">
   Deep Learning Problems and Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5.%20Deep%20Learning/3.%20Convolution%20Neural%20Networks.html">
   Convolutional Neural Networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Science for Finance
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../6.%20Data%20Science%20for%20Finance/Financial%20Data.html">
   Financial Data
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Science and Sustainability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../7.%20Data%20Science%20and%20Sustainability/Sustainable%20Data.html">
   Sustainable Data
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/3. Supervised ML/Classification.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/3. Supervised ML/Classification.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-classification">
   What is Classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-does-a-classification-model-work">
   How does a classification model work?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-loss-function">
     The loss function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-the-model">
     1. Training the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predict-new-values">
     2. Predict new values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-are-the-mains-challenges">
     What are the mains challenges
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-to-implement-classification-with-python">
   How to implement classification with python :
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-the-data">
     Load the data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#xgboost-classifier">
     XGBoost Classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#catboost-classifier">
     CatBoost Classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#knn-k-nearest-neighbors">
     KNN (K Nearest Neighbors)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-is-knn">
       What is KNN
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#knn-with-python">
       KNN with python
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Classification</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-classification">
   What is Classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-does-a-classification-model-work">
   How does a classification model work?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-loss-function">
     The loss function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-the-model">
     1. Training the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predict-new-values">
     2. Predict new values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-are-the-mains-challenges">
     What are the mains challenges
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-to-implement-classification-with-python">
   How to implement classification with python :
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-the-data">
     Load the data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#xgboost-classifier">
     XGBoost Classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#catboost-classifier">
     CatBoost Classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#knn-k-nearest-neighbors">
     KNN (K Nearest Neighbors)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-is-knn">
       What is KNN
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#knn-with-python">
       KNN with python
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="classification">
<h1>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">#</a></h1>
<style>body {text-align: justify}</style>
<section id="what-is-classification">
<h2>What is Classification<a class="headerlink" href="#what-is-classification" title="Permalink to this headline">#</a></h2>
<p>Classification consists in taking a vector of inputs X to produce a vector of outputs Y that will associate each input x to a class. The training data set is composed of the vector X of n observations (that can include d parameters), and Y the vector the associated true values. Dataset in the case of <em>supervised</em> machine learning : <span class="math notranslate nohighlight">\( \mathcal{D} = \{(X_1,Y_1), ... ,(X_n,Y_n) \} \in \mathcal{X}^n, \mathcal{Y}^n \)</span>.</p>
<p>The classification algorithm will try to minimize a loss function in order to create a model that can predict the class of each entry with a certain level of precision. During the training phase, the true output is already defined and is given to the algorithm so it can learn how to differenciate those classes. Once the model is trained, it is supposed to be able to correctly predict those output for new values.</p>
<p>An example of classification could be to predict the added value of a new client : small, average, important. The input could include the type of client (individual or company), the location of the store in which they made their first purchase, the amount of their first purchase, the day of the week… The model will transform the input vector in a value corresponding to one of the classes. During the training phase, you should take data that already exists.</p>
</section>
<section id="how-does-a-classification-model-work">
<h2>How does a classification model work?<a class="headerlink" href="#how-does-a-classification-model-work" title="Permalink to this headline">#</a></h2>
<p>Once the data is correclty collected, cleaned and formated, you can proceed to develop and train a classification model. There are several algorithm that will help you to do this task. However, it is important to understand how they work under the hood, and the right way to use them.</p>
<section id="the-loss-function">
<h3>The loss function<a class="headerlink" href="#the-loss-function" title="Permalink to this headline">#</a></h3>
<p>As specified in the graphs above, one of the main part of the training process consist in comparing the predicted values to the true output. For a classification problem, this comparison is done by a loss function that can be defined as :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{l}(Y,f(X)) = \sum_{i=1}^{n} 1_{Y_i \ne f(X_i)}
\]</div>
<p><span class="math notranslate nohighlight">\(1_{Y_i \ne f(X_i)}\)</span> is equal to <span class="math notranslate nohighlight">\(1\)</span> when <span class="math notranslate nohighlight">\(Y_i \ne prediction\)</span>. Therefore, for each mistake that the model makes, the loss function is incremented of one unit. A model that correctly predicts every observation has a loss of 0, and a model that is always wrong has a loss equal to the number of entries in the dataset. We aim a finding a function <span class="math notranslate nohighlight">\(f\)</span> such that <span class="math notranslate nohighlight">\(\mathcal{l}(Y,f(X))\)</span> gets as close as possible to 0.</p>
<p>Here is a simple example of a classification model :</p>
</section>
<section id="training-the-model">
<h3>1. Training the model<a class="headerlink" href="#training-the-model" title="Permalink to this headline">#</a></h3>
<center>
<img src="pictures/classificationTraining.png">
</center>
</section>
<section id="predict-new-values">
<h3>2. Predict new values<a class="headerlink" href="#predict-new-values" title="Permalink to this headline">#</a></h3>
<center>
<img src="./pictures/classificationPredictions.png">
</center>
</section>
<section id="what-are-the-mains-challenges">
<h3>What are the mains challenges<a class="headerlink" href="#what-are-the-mains-challenges" title="Permalink to this headline">#</a></h3>
<p>As classification greatly depends on the quality of the input, therefore the first challenge we encounter is to feed the model with the proper data and the right amount of information. Indeed, giving too much information to the model can lead to over-fitting, especially if the information is not independ. It will also take more time to train the model and to get precise results. On the other hand, by not giving enough data to the model, you won’t be able to train it correctly and it will never reach a correct level of precision.</p>
</section>
</section>
<section id="how-to-implement-classification-with-python">
<h2>How to implement classification with python :<a class="headerlink" href="#how-to-implement-classification-with-python" title="Permalink to this headline">#</a></h2>
<p>In this example, I will import the necessary packages as we go. This is supposed to help you understand in which situation and part of the process the different packages are useful. However, in reality, a good practice is to import all the packages in the first cell of the notebook.</p>
<section id="load-the-data">
<h3>Load the data<a class="headerlink" href="#load-the-data" title="Permalink to this headline">#</a></h3>
<p>Pandas and numpy are the main packages used for loading/creating data sets. However, in a real case scenario, you might need a specific package to access a data base (boto3, sql…) or to do data transformation. Then, download the data set : <a class="reference external" href="https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset/download?datasetVersionNumber=1">https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset/download?datasetVersionNumber=1</a> and unzip the file and load it in python with pandas.</p>
<p>Once the dataset has correctly been loaded, it is necessary to spend time analysing it and manipulating the data in order to get a deep understanding of what is at stake. The previous chapter deals with how to clean and visualize the data. Here, the dataset is already clean enough and well structured for us to do a classification. Therefore, it is possible to start working on it directly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 

<span class="n">train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../datasets/trees/train.csv&quot;</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../datasets/trees/test.csv&quot;</span><span class="p">)</span>

<span class="n">train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div class="cell tag_remove-input tag_output-scroll tag_output_scroll docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Id</th>
      <th>Elevation</th>
      <th>Aspect</th>
      <th>Slope</th>
      <th>Horizontal_Distance_To_Hydrology</th>
      <th>Vertical_Distance_To_Hydrology</th>
      <th>Horizontal_Distance_To_Roadways</th>
      <th>Hillshade_9am</th>
      <th>Hillshade_Noon</th>
      <th>Hillshade_3pm</th>
      <th>...</th>
      <th>Soil_Type32</th>
      <th>Soil_Type33</th>
      <th>Soil_Type34</th>
      <th>Soil_Type35</th>
      <th>Soil_Type36</th>
      <th>Soil_Type37</th>
      <th>Soil_Type38</th>
      <th>Soil_Type39</th>
      <th>Soil_Type40</th>
      <th>Cover_Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>242642</td>
      <td>2881</td>
      <td>130</td>
      <td>22</td>
      <td>210</td>
      <td>54</td>
      <td>1020</td>
      <td>250</td>
      <td>221</td>
      <td>88</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>309891</td>
      <td>3005</td>
      <td>351</td>
      <td>14</td>
      <td>242</td>
      <td>-16</td>
      <td>1371</td>
      <td>194</td>
      <td>215</td>
      <td>159</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>287847</td>
      <td>3226</td>
      <td>63</td>
      <td>14</td>
      <td>618</td>
      <td>2</td>
      <td>1092</td>
      <td>232</td>
      <td>210</td>
      <td>107</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>516307</td>
      <td>3298</td>
      <td>317</td>
      <td>8</td>
      <td>661</td>
      <td>60</td>
      <td>752</td>
      <td>198</td>
      <td>233</td>
      <td>174</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>124860</td>
      <td>3080</td>
      <td>35</td>
      <td>6</td>
      <td>175</td>
      <td>26</td>
      <td>3705</td>
      <td>219</td>
      <td>227</td>
      <td>144</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 56 columns</p>
</div></div></div>
</div>
<div class="cell tag_remove-input docutils container">
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Cover_Type&#39;</span><span class="p">,</span><span class="s1">&#39;Id&#39;</span><span class="p">]),</span> <span class="n">train</span><span class="o">.</span><span class="n">Cover_Type</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y_train_encode</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_test_encode</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="xgboost-classifier">
<h3>XGBoost Classifier<a class="headerlink" href="#xgboost-classifier" title="Permalink to this headline">#</a></h3>
<p>XGBoost is one of the most popular <span class="math notranslate nohighlight">\(\textbf{gradient boosting}\)</span> library and is particularly efficient when it comes to classification. It relies on a boosting technic, which belongs to ensemble technics, that consists in training different model sequentially so that a specific tree can learn from the mistakes made by the previous ones.</p>
<p>More precisely, <span class="math notranslate nohighlight">\(\textbf{boosting}\)</span> is a machine learning technique that combines multiple weak learners, i.e. simple models that are slightly better than random guessing, to create a single strong learner (complex model). It is an iterative process, during which each weak learner is trained on the mistakes of the previous weak learner. At the end of the process, the weak learners are then combined to create a final strong learner which is more accurate and powerful than any of the individual weak learners. Boosting is often used in ensemble learning methods to improve the accuracy of predictions, and models are added until there is no room left for improvement.</p>
<p>Gradient boosting is an iterative technique based on boosting for optimizing a loss function by combining the output of multiple weak models. It works by sequentially adding a new model to the ensemble, each of which is trained to minimize the loss of the combined model on the training set. Let <span class="math notranslate nohighlight">\(f_1(x), ..., f_M(x)\)</span> be weak models, and <span class="math notranslate nohighlight">\(F_m(x)\)</span> the combined model after <span class="math notranslate nohighlight">\(m\)</span> weak models have been added. The combined model, <span class="math notranslate nohighlight">\(F_m(x)\)</span>, is updated at each iteration <span class="math notranslate nohighlight">\(m\)</span> using the following formula:</p>
<div class="math notranslate nohighlight">
\[F_m(x) = F_{m-1}(x) + \alpha f_m(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is a parameter controlling the contribution of the new model. The parameter <span class="math notranslate nohighlight">\(\alpha\)</span> is learned by minimizing an objective function, usually the mean squared error, on the training set:</p>
<div class="math notranslate nohighlight">
\[
\alpha = \underset{\alpha}{\operatorname{argmin}} \sum_{i=1}^n \left[y_i - F_{m-1}(x_i) - \alpha f_m(x_i) \right]^2
\]</div>
<p>XGBoost works by using the gradient boosting technique, which is a sequential approach to building a predictive model. This technique is used to optimize a loss function by adding weak learners in a step-wise fashion. The algorithm works by first fitting a simple model to the data, then sequentially adding more weak learners to the model and updating the model parameters in order to minimize the loss function. Mathematically, XGBoost works by minimizing the following objective function:</p>
<div class="math notranslate nohighlight">
\[
\underset{\theta}{min}\sum_{i=1}^{n}\left[ l\left(y_{i},\hat{y_{i}}\right) + \sum_{j=1}^{K}\Omega\left(f_{j}\right)\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the target value for the <span class="math notranslate nohighlight">\(i^{th}\)</span> data point, <span class="math notranslate nohighlight">\(\hat{y_i}\)</span> is the predicted value, <span class="math notranslate nohighlight">\(K\)</span> is the number of weak learners, <span class="math notranslate nohighlight">\(f_j\)</span> is the <span class="math notranslate nohighlight">\(j^{th}\)</span> weak learner, and <span class="math notranslate nohighlight">\(\Omega\)</span> is the regularization term. The first term in the objective is the loss function which measures the discrepancy between the actual and predicted values. The second term is the regularization term which penalizes complex models.</p>
<p>You can learn more here : <a class="reference external" href="https://arxiv.org/pdf/1603.02754.pdf">https://arxiv.org/pdf/1603.02754.pdf</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>

<span class="n">XGBClassifier</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">()</span>
<span class="n">XGBClassifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_encode</span><span class="p">)</span>

<span class="n">output_train</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">output_test</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train_encode</span><span class="p">,</span> <span class="n">output_train</span><span class="p">)</span>
<span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_encode</span><span class="p">,</span> <span class="n">output_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The accuracy on the train set is equal to </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">acc_train</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The accuracy on the test set is equal to </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">acc_test</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The accuracy on the train set is equal to 99.0%
The accuracy on the test set is equal to 85.4%
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;max_depth&quot;</span><span class="p">:</span>    <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">],</span>
              <span class="s2">&quot;n_estimators&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">],</span>
              <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">0.1</span><span class="p">]}</span>

<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">XGBClassifier</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_encode</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The best hyperparameters are &quot;</span><span class="p">,</span><span class="n">search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bestXGBClassifier</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span> <span class="mi">9</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span> <span class="mi">300</span><span class="p">,</span><span class="n">reg_alpha</span><span class="o">=</span><span class="mi">1</span> <span class="p">)</span> 
<span class="n">bestXGBClassifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_encode</span><span class="p">)</span>

<span class="n">output_train</span> <span class="o">=</span> <span class="n">bestXGBClassifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">output_test</span> <span class="o">=</span> <span class="n">bestXGBClassifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train_encode</span><span class="p">,</span> <span class="n">output_train</span><span class="p">)</span>
<span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_encode</span><span class="p">,</span> <span class="n">output_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The accuracy on the train set is equal to </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">acc_train</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The accuracy on the test set is equal to </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">acc_test</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>

<span class="n">bestXGBClassifier</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s2">&quot;bestXGBClassifier.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt 
fig, ax = plt.subplots(1,1,figsize=(20,10))
xgb.plot_importance(XGBClassifier, ax = ax, height=0.4,max_num_features =12)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Classification_12_0.png" src="../_images/Classification_12_0.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>Cover_Type = XGBClassifier.predict(test.drop(columns=[&quot;Id&quot;]))
predictions = pd.DataFrame({&quot;Id&quot;:test.Id,&quot;Predicted_Cover_Type&quot;:Cover_Type})
predictions[&quot;Predicted_Cover_Type&quot;] = le.inverse_transform(predictions[&quot;Predicted_Cover_Type&quot;])
#predictions.to_csv(&quot;data/XGBClassifierPredictions.csv&quot;,index=False)
predictions
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Id</th>
      <th>Predicted_Cover_Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>581007</th>
      <td>581008</td>
      <td>3</td>
    </tr>
    <tr>
      <th>581008</th>
      <td>581009</td>
      <td>3</td>
    </tr>
    <tr>
      <th>581009</th>
      <td>581010</td>
      <td>3</td>
    </tr>
    <tr>
      <th>581010</th>
      <td>581011</td>
      <td>3</td>
    </tr>
    <tr>
      <th>581011</th>
      <td>581012</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>581012 rows × 2 columns</p>
</div></div></div>
</div>
<p>Once your XGBoost model is trained, you can dump a human readable description of it into a text file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">XGBClassifier</span><span class="o">.</span><span class="n">dump_model</span><span class="p">(</span><span class="s1">&#39;dump.raw.txt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="catboost-classifier">
<h3>CatBoost Classifier<a class="headerlink" href="#catboost-classifier" title="Permalink to this headline">#</a></h3>
<p>CATBoost is another popular open-source machine learning algorithm based on gradient boosting, that was developed by Yandex in 2017. The main difference between XGBoost and CATboost is that XGBoost is a general-purpose boosting algorithm, while CATboost is a specialized algorithm designed to handle categorical features. CATboost is in a way a specialized version of XGBoost which has been optimized for working with categorical features. It uses a technique called ordered boosting, which is specifically designed to handle categorical features. This allows it to better handle data with high cardinality and missing values. It also uses a specialized algorithm for calculating the feature importance, which is better suited for categorical features.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">CatBoostClassifier</span>

<span class="n">CATBClassifier</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span><span class="n">silent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">CATBClassifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_encode</span><span class="p">)</span>

<span class="n">output_train</span> <span class="o">=</span> <span class="n">CATBClassifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">output_test</span> <span class="o">=</span> <span class="n">CATBClassifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train_encode</span><span class="p">,</span> <span class="n">output_train</span><span class="p">)</span>
<span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_encode</span><span class="p">,</span> <span class="n">output_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The accuracy on the train set is equal to </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">acc_train</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The accuracy on the test set is equal to </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">acc_test</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The accuracy on the train set is equal to 94.8%
The accuracy on the test set is equal to 84.7%
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>feature_importance = CATBClassifier.feature_importances_[:12]
sorted_idx = np.argsort(feature_importance)
fig = plt.figure(figsize=(12, 6))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align=&#39;center&#39;)
plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])
plt.title(&#39;Feature Importance&#39;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Classification_18_0.png" src="../_images/Classification_18_0.png" />
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;random_strength&quot;</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span>
              <span class="s2">&quot;l2_leaf_reg&quot;</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span>
              <span class="s2">&quot;max_depth&quot;</span><span class="p">:[</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">],</span>
              <span class="s2">&quot;learning_rate&quot;</span><span class="p">:[</span><span class="mf">0.17</span><span class="p">,</span><span class="mf">0.23</span><span class="p">]}</span>

<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">CATBClassifier</span> <span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_encode</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The best hyperparameters are &quot;</span><span class="p">,</span><span class="n">search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="n">BestCATBClassifier</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span> <span class="mi">10</span><span class="p">,</span><span class="n">l2_leaf_reg</span><span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">random_strength</span><span class="o">=</span> <span class="mi">3</span><span class="p">,</span><span class="n">silent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">BestCATBClassifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_encode</span><span class="p">)</span>

<span class="n">output_train</span> <span class="o">=</span> <span class="n">BestCATBClassifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">output_test</span> <span class="o">=</span> <span class="n">BestCATBClassifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train_encode</span><span class="p">,</span> <span class="n">output_train</span><span class="p">)</span>
<span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_encode</span><span class="p">,</span> <span class="n">output_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The accuracy on the train set is equal to </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">acc_train</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The accuracy on the test set is equal to </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">acc_test</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="knn-k-nearest-neighbors">
<h3>KNN (K Nearest Neighbors)<a class="headerlink" href="#knn-k-nearest-neighbors" title="Permalink to this headline">#</a></h3>
<section id="what-is-knn">
<h4>What is KNN<a class="headerlink" href="#what-is-knn" title="Permalink to this headline">#</a></h4>
<p>K Nearest Neighbors (KNN) is a supervised machine learning algorithm that can be used for both classification and regression tasks. It is a non-parametric algorithm meaning that it does not make assumptions about the underlying data distribution. The KNN algorithm works by finding the ‘K’ closest data points in the training data set and then using them to make predictions about new data points. The prediction is based on the majority of the ‘K’ closest data points.</p>
<p>To classify a new data point, KNN finds the ‘K’ closest data points in the training set and determines the class of the new data point based on the majority vote of the ‘K’ closest points. For regression tasks, the KNN algorithm finds the ‘K’ closest data points and then calculates the average value of the ‘K’ closest points to predict the value of the new data point.</p>
<p>KNN is a simple yet powerful algorithm for both classification and regression tasks. It is easy to understand and implement and does not require any assumptions about the data distribution. It also does not require much tuning of the parameters, making it an attractive option for many machine learning tasks.</p>
<p>However, the KNN algorithm does have some drawbacks. It can be computationally expensive and can be sensitive to outliers in the data set. It also tends to overfit data when the number of neighbors is too large. Despite these drawbacks, KNN remains a popular choice for many machine learning tasks.</p>
<p>Let <span class="math notranslate nohighlight">\(x_i\)</span> be a training data point with <span class="math notranslate nohighlight">\(n\)</span> features and let <span class="math notranslate nohighlight">\(y_i\)</span> be the corresponding label. For an unseen test point <span class="math notranslate nohighlight">\(x_j\)</span>, the KNN algorithm assigns the label <span class="math notranslate nohighlight">\(y_j\)</span> according to the following formula:</p>
<div class="math notranslate nohighlight">
\[
y_j = \frac{1}{k}\sum_{i=1}^{k}y_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the number of nearest neighbors. The distance between a training point <span class="math notranslate nohighlight">\(x_i\)</span> and the test point <span class="math notranslate nohighlight">\(x_j\)</span> is computed using a distance metric such as Euclidean distance. The KNN algorithm assigns the label of the most common class among the k-nearest neighbors.</p>
<p>Recall that the Euclidean distance between p and q in <span class="math notranslate nohighlight">\(n\)</span> dimensions is defined as:</p>
<div class="math notranslate nohighlight">
\[
d(p,q) = \sqrt{\sum_{i=1}^{n} (q_i - p_i)^2}
\]</div>
<div class="math notranslate nohighlight">
\[
\text{with $q_i - p_i$ the distance between the $i^{th}$ feature}
\]</div>
<p><em>Note : in 2 dimensions, it corresponds to the pythagorian theorem.</em></p>
</section>
<section id="knn-with-python">
<h4>KNN with python<a class="headerlink" href="#knn-with-python" title="Permalink to this headline">#</a></h4>
<p>Now let’s see how to implement KNN in Python. First, we create a dataset with 3 features and 3 classes. This can represent 3 groups of customers, each of wich is based on 3 paramters, the age, the location, and the time since the last purchase for instance. This is our labeled data. We know want to predict in wich group a new customer will be. This new customer is represented in yellow below.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from collections import Counter
from sklearn.datasets import make_classification

#Here we will work only with X in 3 dimensions, with only 3 useful features 
X,y = make_classification(n_samples=200,n_features=3, n_informative=3, n_redundant=0, n_classes=3,n_clusters_per_class=1,class_sep=5,random_state=3) 
X[:,0] = X[:,0].astype(&#39;int&#39;)+30
X[:,1] = X[:,1].astype(&#39;int&#39;)
X[:,2] = X[:,2].astype(&#39;int&#39;)+10

data = pd.DataFrame({&quot;Age&quot;:X[:,0],&quot;Location&quot;:X[:,1],&quot;Last purchase&quot;:X[:,2],&quot;class&quot;:y})
print(&quot;The dataset containing information about customers: &quot;)
data.head()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The dataset containing information about customers: 
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Location</th>
      <th>Last purchase</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>35.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>36.0</td>
      <td>4.0</td>
      <td>9.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>36.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>25.0</td>
      <td>4.0</td>
      <td>5.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35.0</td>
      <td>5.0</td>
      <td>15.0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="c1"># axes instance</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">auto_add_to_figure</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>

<span class="n">cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;#FF0000&quot;</span><span class="p">,</span> <span class="s2">&quot;#00FF00&quot;</span><span class="p">,</span> <span class="s2">&quot;#0000FF&quot;</span><span class="p">])</span>
<span class="c1"># plot</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;yellow&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">155</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Age&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Location&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Weeks since last purchase&#39;</span><span class="p">)</span>

<span class="c1"># legend</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="o">*</span><span class="n">sc</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">(),</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Dataset in 3D space&quot;</span><span class="p">)</span>
<span class="c1"># save</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/Classification_24_0.png" src="../_images/Classification_24_0.png" />
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># KNN Model from scratch : </span>

<span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>

<span class="k">class</span> <span class="nc">KNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="c1"># We start by computing the distances </span>
        <span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_train</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_train</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">]</span>

        <span class="c1"># Then we sort by distance </span>
        <span class="n">k_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span>
        <span class="n">k_neighbor_labels</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">k_idx</span><span class="p">]</span>

        <span class="c1"># return the most common class label</span>
        <span class="n">most_common</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">k_neighbor_labels</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">most_common</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="cell tag_remove-input docutils container">
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># KNN Model with Sklearn, and comparison :</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>

<span class="n">knn_model_Scratch</span> <span class="o">=</span> <span class="n">KNN</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">knn_model_SKlearn</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>

<span class="n">knn_model_Scratch</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">knn_model_SKlearn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">new_customer_data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">32</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)]</span>

<span class="n">predictions_Scratch</span> <span class="o">=</span> <span class="n">knn_model_Scratch</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_customer_data</span><span class="p">)</span>
<span class="n">predictions_SKlearn</span> <span class="o">=</span> <span class="n">knn_model_SKlearn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_customer_data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The model from scratch predicted the class: &quot;</span><span class="p">,</span> <span class="n">predictions_Scratch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The model from sklearn predicted the class: &quot;</span><span class="p">,</span> <span class="n">predictions_SKlearn</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The accuracy of scratch model is: &quot;</span><span class="p">,</span> 
    <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">knn_model_Scratch</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The accuracy of sklearn model is: &quot;</span><span class="p">,</span> 
    <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">knn_model_SKlearn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</pre></div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The model from scratch predicted the class:  [0]
The model from sklearn predicted the class:  [0.]
The accuracy of scratch model is:  1.0
The accuracy of sklearn model is:  1.0
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3. Supervised ML"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Regression.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Regression</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../4.%20Unsupervised%20ML/1.%20Dimension%20Reduction.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Unsupervised Machine Learning and Dimension Reduction</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Guillaume de Surville<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>