
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Introduction to Neural Networks &#8212; Data Science for Business</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Deep Learning Problems and Optimization" href="2.%20Deep%20Learning%20Optimization.html" />
    <link rel="prev" title="Classification" href="../4.%20Unsupervised%20ML/Classification.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/datascience.webp" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science for Business</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Data Science for business
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Python and Data Science
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../1.%20Intro/What%20is%20data%20science.html">
   What is Data Science ?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1.%20Intro/What%20is%20python.html">
   What is python ?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1.%20Intro/Presentation%20of%20this%20blog.html">
   Presentation of this Blog
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Manipulation and Data Visualization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2.%20Data%20visualisation/Data%20manipulation.html">
   Data Manipulation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Supervised Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../3.%20Supervised%20ML/Regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3.%20Supervised%20ML/Classification.html">
   Classification
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unsupervised Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4.%20Unsupervised%20ML/Classification.html">
   Classification
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Learning and Optimization
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction to Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2.%20Deep%20Learning%20Optimization.html">
   Deep Learning Problems and Optimization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Science for Finance
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../6.%20Data%20Science%20for%20Finance/Financial%20Data.html">
   Financial Data
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Science and Sustainability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../7.%20Data%20Science%20and%20Sustainability/Sustainable%20Data.html">
   Sustainable Data
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/5. Deep Learning/1. Introduction to Neural Networks.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/5. Deep Learning/1. Introduction to Neural Networks.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-a-neuron">
   1. What is a neuron?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mcculloch-and-pitts-neuron-1943">
     McCulloch and Pitts neuron - 1943
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-perceptron-1958">
     The perceptron - 1958
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-the-right-parameters">
     Finding the right parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adaline-and-madaline-1959">
     ADALINE and MADALINE - 1959
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backpropagation-how-to-find-the-weights-and-biais">
   2. Backpropagation : How to find the weights and biais?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-are-we-trying-to-do">
     What are we trying to do ?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-backpropagation">
     What is backpropagation ?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparameters-of-a-neural-network">
   3. Hyperparameters of a neural network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-are-the-hyperparameters">
     What are the hyperparameters ?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#main-activation-functions">
     Main activation functions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sigmoid-function">
       1. Sigmoid function
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hyperbolic-tangent">
       2. Hyperbolic tangent
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#rectified-linear-unit-relu">
       3. Rectified Linear Unit (ReLU)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#parametric-relu">
       4. Parametric ReLU
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exponential-linear-unit-elu">
       5. Exponential Linear Unit (ELU)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#maxout">
       6. Maxout
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#swich-function">
       7. Swich function
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#output-functions">
     Output functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-functions">
     Loss Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization">
   4. Regularization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#penalization">
     Penalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropout">
     Dropout
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch-normalization">
     Batch normalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#early-stopping">
     Early stopping
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-conclusion">
   5. In conclusion
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Introduction to Neural Networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-a-neuron">
   1. What is a neuron?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mcculloch-and-pitts-neuron-1943">
     McCulloch and Pitts neuron - 1943
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-perceptron-1958">
     The perceptron - 1958
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-the-right-parameters">
     Finding the right parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adaline-and-madaline-1959">
     ADALINE and MADALINE - 1959
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backpropagation-how-to-find-the-weights-and-biais">
   2. Backpropagation : How to find the weights and biais?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-are-we-trying-to-do">
     What are we trying to do ?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-backpropagation">
     What is backpropagation ?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparameters-of-a-neural-network">
   3. Hyperparameters of a neural network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-are-the-hyperparameters">
     What are the hyperparameters ?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#main-activation-functions">
     Main activation functions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sigmoid-function">
       1. Sigmoid function
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hyperbolic-tangent">
       2. Hyperbolic tangent
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#rectified-linear-unit-relu">
       3. Rectified Linear Unit (ReLU)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#parametric-relu">
       4. Parametric ReLU
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exponential-linear-unit-elu">
       5. Exponential Linear Unit (ELU)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#maxout">
       6. Maxout
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#swich-function">
       7. Swich function
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#output-functions">
     Output functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-functions">
     Loss Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization">
   4. Regularization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#penalization">
     Penalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropout">
     Dropout
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch-normalization">
     Batch normalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#early-stopping">
     Early stopping
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-conclusion">
   5. In conclusion
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="introduction-to-neural-networks">
<h1>Introduction to Neural Networks<a class="headerlink" href="#introduction-to-neural-networks" title="Permalink to this headline">#</a></h1>
<div style="text-align: justify">
This is a first introduction to Neural Networks. It aims at presenting the main ideas, concept and element of vocabulary needed to understand this type of algorithm and to be able to implement them in a real world situation. 
<br></br>
In science, there are two main approches to simulate intelligent behaviour : 
<ul>
<li>Connectionism : using connected circuit that form neural networks </li>
<li>Symbolic AI : combining human readable representations of problems </li>
</ul>
Connectionism led to the development of modern neural networks, which are based on ideas that were developed as early as the 1940s. 
</div><section id="what-is-a-neuron">
<h2>1. What is a neuron?<a class="headerlink" href="#what-is-a-neuron" title="Permalink to this headline">#</a></h2>
<div style="text-align: justify">
A neuron is a basic unit in a neural network composed of an input layer, an output layer, and one or more hidden layers. It is responsible for processing information and passing the results to other neurons in the network. Neurons are connected to each other by weighted connections, which can be adjusted to allow the neurons to learn. Neurons are the fundamental building blocks of a neural network, and they are responsible for making decisions, predictions, and learning from experiences.
</div><section id="mcculloch-and-pitts-neuron-1943">
<h3>McCulloch and Pitts neuron - 1943<a class="headerlink" href="#mcculloch-and-pitts-neuron-1943" title="Permalink to this headline">#</a></h3>
<center>
<img src="pictures/neuron.png">
</center>
</section>
<section id="the-perceptron-1958">
<h3>The perceptron - 1958<a class="headerlink" href="#the-perceptron-1958" title="Permalink to this headline">#</a></h3>
<div style="text-align: justify">
The perceptron algorithm was invented in 1958 by Frank Rosenblatt and is the foundation of modern neural networks. It is an algorithm for supervised learning of binary classifier that is composed of a single-layer feed-forward neural network that uses a step function as its activation function. It can be used for supervised learning, where the output is known and the algorithm is trained to predict the correct output for a given input. The perceptron is a linear classifier, meaning it assigns a binary label to each input vector. The perceptron algorithm is often used in supervised learning tasks such as image recognition and natural language processing.
</div>
<center>
<img src="pictures/perceptron.png">
</center>
</section>
<section id="finding-the-right-parameters">
<h3>Finding the right parameters<a class="headerlink" href="#finding-the-right-parameters" title="Permalink to this headline">#</a></h3>
<div style="text-align: justify">
For each of these neurons, a vector of containing the observations and the biais is passed as the input. For every element of the vector, there is a corresponding weight that will be passed in the activation function with the observation. In order to tune the neuron, we have to estimate the vector of weights by proceeding as follow : 
</div>
<p>We have the data set <span class="math notranslate nohighlight">\(D_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[
D_n = {(X_i,y_i)_{i \in [1,n]}} 
\]</div>
<p>With :</p>
<ul class="simple">
<li><p>The input data of size d : <span class="math notranslate nohighlight">\( {X_i} \in R^n \)</span></p></li>
<li><p>The output : <span class="math notranslate nohighlight">\( {y_i} \in \{-1,1\} \)</span></p></li>
<li><p>The vector of weights and biaises : <span class="math notranslate nohighlight">\( \tilde{w} = (w_1, ... , w_n, b)\)</span></p></li>
</ul>
<p>How do we estimate <span class="math notranslate nohighlight">\(\tilde{w}\)</span> ?</p>
<ol class="simple">
<li><p>Start with : <span class="math notranslate nohighlight">\(\tilde{w} = 0\)</span></p></li>
<li><p>If : <span class="math notranslate nohighlight">\( \hspace{0.2cm} y_i &lt;\tilde{w},\tilde{x_i}&gt;  \hspace{0.2cm} \le 0 \hspace{0.2cm} \implies \hspace{0.2cm} y_i \ne pred(y_i) \)</span>
<br> Then : <span class="math notranslate nohighlight">\( \tilde{w} \Leftarrow \tilde{w} + y_i \tilde{x_i}\)</span>
<br> Else  : <span class="math notranslate nohighlight">\( \tilde{w} \Leftarrow \tilde{w}\)</span></p></li>
</ol>
<p>The perceptron algorithm can be seen as a stochastic gradient descent :</p>
<div class="math notranslate nohighlight">
\[ 
\tilde{w}_{t+1} \leftarrow \tilde{w}_t - \eta \nabla_w \mathcal{L}(\tilde{w}_t) 
\]</div>
<p>With the corresponding loss function :</p>
<div class="math notranslate nohighlight">
\[ 
\mathcal{L}(\tilde{w}_t) = -\sum_{i \in [1,n]}y_i &lt;\tilde{w},\tilde{x_i}&gt; 
\]</div>
<p>The theorem of Block and Novikoff in 1963 showed that if the data set is linearly separable, then the number of update k of the perceptron is bounded by : <span class="math notranslate nohighlight">\( k+1 \le \frac{1 + R^2}{\gamma^2} \)</span>, with <span class="math notranslate nohighlight">\( \gamma = \min{y_i &lt;\tilde{w^*},\tilde{x_i}&gt;} \)</span></p>
<div style="text-align: justify">
As a consequence, the perceptron converge only if the data is linearly separable. Therefore, the hopes that the reserchers and the army had placed in the perceptron were shattered and thus began the first "AI winter", in the late 60s. 
</div></section>
<section id="adaline-and-madaline-1959">
<h3>ADALINE and MADALINE - 1959<a class="headerlink" href="#adaline-and-madaline-1959" title="Permalink to this headline">#</a></h3>
<div style="text-align: justify">
Those two algorithms aimed to solve real world problems, with non linearly separable data. They are based on similar neurons as the ones before with a loss function defined as the square diffference between a weighted sum of inputs and the output. The optimization procedure consists in a trivial gradient descent computed before the output function. 
</div>
<center>
<img src="pictures/Adaline.png">
</center></section>
</section>
<section id="backpropagation-how-to-find-the-weights-and-biais">
<h2>2. Backpropagation : How to find the weights and biais?<a class="headerlink" href="#backpropagation-how-to-find-the-weights-and-biais" title="Permalink to this headline">#</a></h2>
<section id="what-are-we-trying-to-do">
<h3>What are we trying to do ?<a class="headerlink" href="#what-are-we-trying-to-do" title="Permalink to this headline">#</a></h3>
<p>In order to have a functional neural network, we aim to minimize the empirical risk :</p>
<center>
<p><span class="math notranslate nohighlight">\( \underset{\theta}{\mathrm{argmin}}[\frac{1}{n}\sum_{i=1}^{n}\mathcal{l}(Y_i,f_\theta(X_i))] \equiv \underset{\theta}{\mathrm{argmin}}[\frac{1}{n}\sum_{i=1}^{n}\mathcal{l}_i(\theta)]\)</span></p>
</center>
<p>We use a stochastic gradient with a rule:</p>
<ul class="simple">
<li><p>Considering <span class="math notranslate nohighlight">\( I_t \subset \{1,n\}\)</span>, while <span class="math notranslate nohighlight">\( \lvert \theta_t - \theta_{t+1} \lvert \ge \epsilon \)</span>:</p></li>
</ul>
<center>
<p><span class="math notranslate nohighlight">\(\theta_{t+1} = \theta_t - \eta_t [\frac{1}{\lvert I_t \lvert}\sum_{i\in I_t}^{n}\nabla_\theta\mathcal{l_i}(\theta_t)]\)</span></p>
</center> </section>
<section id="what-is-backpropagation">
<h3>What is backpropagation ?<a class="headerlink" href="#what-is-backpropagation" title="Permalink to this headline">#</a></h3>
<div style="text-align: justify"> 
The backpropagation helps us to optimize the parameters of our model. It requires a non-zero gradient almost everywhere, wich forbides us to choose a 0-1 loss function. 
The backpropagation help us to compute the gradient more efficiently, but is is not an optimization algorithm. It computes the loss of a gradient based on a batch of data. We do it by computing the gradient recursively. 
<div>
<p>Let us consider a neural network with L layers, a vector output and a quadratic cost equal to :</p>
<center>
<p><span class="math notranslate nohighlight">\( C = \left\lVert \frac{1}{2}{y - a^{(l)}} \right\rVert^2 \)</span></p>
</center>
<p>With <span class="math notranslate nohighlight">\( z_j^{(l)} \)</span> the entry of the neuron <span class="math notranslate nohighlight">\(j\)</span> of the layer <span class="math notranslate nohighlight">\(l\)</span>, we have :</p>
<center>
<p><span class="math notranslate nohighlight">\( \delta_j^{(l)} = \frac{\partial C}{\partial z_j^{(l)}} \)</span></p>
</center>
<p>And the four fundamental equations of backpropagation are given by :</p>
<center>
<p><span class="math notranslate nohighlight">\( \delta^{(L)} = \nabla_a C \otimes \sigma'(z^{(L)}) \)</span></p>
<p><span class="math notranslate nohighlight">\( \delta^{(l)} = ((w^{(l+1)})^T\delta^{(l+1)})\otimes \sigma'(z^{(l)}) \)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial C}{\partial b_j^{(l)}} = \delta_j^{(l)}\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial C}{\partial w_{j,k}^{(l)}} = a_k^{(l-1)} \delta_j^{(l)}\)</span></p>
</center>
<div style="text-align: justify"> 
We then proceed to train the neural network in 4 steps : 
<ol>
<li><strong>Feedfoward</strong> : We send the sample data through the network and store for each neuron the results of the activation function and the derivative.</li>
<li><strong>Output loss</strong>  : We compute the loss. </li>
<li><strong>Backpropagation</strong>  : We compute recursively the vectors of derivative from the last layer L to the first one, using the first two fundamental equations. Then we compute the gradient using the last two equations. </li>
<li><strong>Optimization</strong> : We update the weights and the biaises with a gradient-based optimisation procedure. </li>
</ol>
<div>
<p>We repeat those steps until we reach the desired level of precision.</p>
<center>
<img src="pictures/backpropagation.webp">
</center></section>
</section>
<section id="hyperparameters-of-a-neural-network">
<h2>3. Hyperparameters of a neural network<a class="headerlink" href="#hyperparameters-of-a-neural-network" title="Permalink to this headline">#</a></h2>
<section id="what-are-the-hyperparameters">
<h3>What are the hyperparameters ?<a class="headerlink" href="#what-are-the-hyperparameters" title="Permalink to this headline">#</a></h3>
<p>When constructing a neural network, you have to make various choices concerning the structure of this network. For instance, you will have to choose the number of hidden layers, the number of neurons per layer, and the activation function within the neurons. There are no hard rules for these, but there are guidelines that will help you to make the right choice.</p>
<center>
<img src="pictures/structureNN.png">
</center></section>
<section id="main-activation-functions">
<h3>Main activation functions<a class="headerlink" href="#main-activation-functions" title="Permalink to this headline">#</a></h3>
<p>There are a few common activation functions that are regularly used within the neurons. It is important to know how they work, when you should use them and when you should not.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import matplotlib.pyplot as plt 
</pre></div>
</div>
</div>
</div>
<section id="sigmoid-function">
<h4>1. Sigmoid function<a class="headerlink" href="#sigmoid-function" title="Permalink to this headline">#</a></h4>
<p>This function is very common in data science. However, it has a few drawbacks :</p>
<ul class="simple">
<li><p>The gradient is close to 0 at the asymptotes</p></li>
<li><p>It is not a zero-centered function, which means that you need to rescale the data</p></li>
<li><p>Compute an exponential is costly</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y = \frac{e^x}{1+e^x}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = np.random.uniform(-5,5,100)
Y = np.exp(X)/(1+np.exp(X))
plt.grid()
plt.plot(np.sort(X),np.sort(Y))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x1365f8280&gt;]
</pre></div>
</div>
<img alt="../_images/1. Introduction to Neural Networks_19_1.png" src="../_images/1. Introduction to Neural Networks_19_1.png" />
</div>
</div>
</section>
<section id="hyperbolic-tangent">
<h4>2. Hyperbolic tangent<a class="headerlink" href="#hyperbolic-tangent" title="Permalink to this headline">#</a></h4>
<p>This function is similar to the sigmoid, but is centered.</p>
<div class="math notranslate nohighlight">
\[
y = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = np.sort(np.random.uniform(-5,5,100))
Y = (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))
plt.grid()
plt.plot(X,Y)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x136a00820&gt;]
</pre></div>
</div>
<img alt="../_images/1. Introduction to Neural Networks_21_1.png" src="../_images/1. Introduction to Neural Networks_21_1.png" />
</div>
</div>
</section>
<section id="rectified-linear-unit-relu">
<h4>3. Rectified Linear Unit (ReLU)<a class="headerlink" href="#rectified-linear-unit-relu" title="Permalink to this headline">#</a></h4>
<p>This function is very efficient from a computational point of view, and the convergence is usually faster than for the previous two functions. However, it is saturated for <span class="math notranslate nohighlight">\( x \le 0 \)</span>.</p>
<div class="math notranslate nohighlight">
\[
Y = \max(0,X)
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = np.sort(np.random.uniform(-5,5,50))
Y = np.fmax(np.zeros(50),X)
plt.grid()
plt.plot(X,Y)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x136a6b400&gt;]
</pre></div>
</div>
<img alt="../_images/1. Introduction to Neural Networks_23_1.png" src="../_images/1. Introduction to Neural Networks_23_1.png" />
</div>
</div>
</section>
<section id="parametric-relu">
<h4>4. Parametric ReLU<a class="headerlink" href="#parametric-relu" title="Permalink to this headline">#</a></h4>
<p>This function is a variation from the ReLU, that has non-zeros values for <span class="math notranslate nohighlight">\( x &lt; 0 \)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y = max\left(\alpha \cdot x, x \right) \\ \text{with : } \alpha &lt;1
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = np.sort(np.random.uniform(-5,5,1000))
alpha = 0.08
Y = np.fmax(alpha*X,X)
plt.grid()
plt.plot(X,Y)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x136b000a0&gt;]
</pre></div>
</div>
<img alt="../_images/1. Introduction to Neural Networks_25_1.png" src="../_images/1. Introduction to Neural Networks_25_1.png" />
</div>
</div>
</section>
<section id="exponential-linear-unit-elu">
<h4>5. Exponential Linear Unit (ELU)<a class="headerlink" href="#exponential-linear-unit-elu" title="Permalink to this headline">#</a></h4>
<p>This variation of the ReLU is differentiable in every point. It is also robust to noise.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y = \left\{ \begin{array}{ll}
x &amp; \text{if } x \geq 0 \\ 
\alpha \left( \exp \left( x \right) - 1 \right) &amp; \text{otherwise}
\end{array} \right. 
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = np.sort(np.random.uniform(-5,5,1000))
alpha = 0.5
Y = [x if x &gt;= 0 else alpha*(np.exp(x)-1) for x in X]
plt.grid()
plt.plot(X,Y)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x136b71240&gt;]
</pre></div>
</div>
<img alt="../_images/1. Introduction to Neural Networks_27_1.png" src="../_images/1. Introduction to Neural Networks_27_1.png" />
</div>
</div>
</section>
<section id="maxout">
<h4>6. Maxout<a class="headerlink" href="#maxout" title="Permalink to this headline">#</a></h4>
<p>This function has no saturation. However, you need to tune k parameters.</p>
<div class="math notranslate nohighlight">
\[
y = max\left(\alpha \cdot x, \beta \cdot x,\delta \cdot x \right)
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = np.sort(np.random.uniform(-5,5,1000))
Y1 = 1.2*X
Y2 = -0.5*X-0.5
Y3 = 0.5*X+1
Y = np.maximum.reduce([Y1,Y2,Y3])
plt.grid()
plt.plot(X,Y)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x136bd9990&gt;]
</pre></div>
</div>
<img alt="../_images/1. Introduction to Neural Networks_29_1.png" src="../_images/1. Introduction to Neural Networks_29_1.png" />
</div>
</div>
</section>
<section id="swich-function">
<h4>7. Swich function<a class="headerlink" href="#swich-function" title="Permalink to this headline">#</a></h4>
<p>This non monotonic function offers a trade off between the ReLU and the linear function.</p>
<div class="math notranslate nohighlight">
\[
y = x \cdot \frac{exp({\beta\cdot x})}{1 + exp({\beta\cdot x})}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = np.sort(np.random.uniform(-5,5,1000))
Y1 = X * (np.exp(0.1 * X)/(1+(np.exp(0.1 * X))))
Y2 = X * (np.exp(1 * X)/(1+(np.exp(1 * X))))
Y3 = X * (np.exp(10 * X)/(1+(np.exp(10 * X))))

plt.grid()
plt.plot(X,Y1)
plt.plot(X,Y2)
plt.plot(X,Y3)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x136c479a0&gt;]
</pre></div>
</div>
<img alt="../_images/1. Introduction to Neural Networks_31_1.png" src="../_images/1. Introduction to Neural Networks_31_1.png" />
</div>
</div>
<p>A rule of thumb consists in using ReLU at first, then Swish, and to avoid sigmoid functions.</p>
</section>
</section>
<section id="output-functions">
<h3>Output functions<a class="headerlink" href="#output-functions" title="Permalink to this headline">#</a></h3>
</section>
<section id="loss-functions">
<h3>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">#</a></h3>
<p>A loss function computes the “level of error” of a neural network. It is defined as such :</p>
<div class="math notranslate nohighlight">
\[
C(x) = \frac{1}{n} \sum_{i=1}^{n} \mathcal{l} (Y_i,f_\theta(X_i))
\]</div>
<p>There are 4 main loss functions :</p>
<ul class="simple">
<li><p><strong>Mean Square Error (MSE)</strong> :</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
C(x) = \frac{1}{n} \sum_{i=1}^{n} (Y_i - f_\theta(X_i))^2
\]</div>
<ul class="simple">
<li><p><strong>Mean Absolute Error</strong> :</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
C(x) = \frac{1}{n} \sum_{i=1}^{n} \vert Y_i - f_\theta(X_i) \vert
\]</div>
<ul class="simple">
<li><p><strong>0 - 1 error</strong> :</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
C(x) = \frac{1}{n} \sum_{i=1}^{n} \textit{1}_{Y_i \ne f_\theta(X_i)}
\]</div>
<ul class="simple">
<li><p><strong>Cross entropy</strong>  :</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
C(x) = -\frac{1}{n} \sum_{i=1}^{n} \left[y_{i} \log{\hat{y}_{i}}\right]
\]</div>
<p>With :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( {y_i} \in \{0,1\} \)</span> the true output</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_{i} \in [0,1] \)</span> the estimated output</p></li>
</ul>
<div style="text-align: justify">The cross entropyt is also called logarithmic loss, log loss or logistic loss. Each predicted class probability is compared to the actual class desired output 0 or 1 and a score/loss is calculated that penalizes the probability based on how far it is from the actual expected value. The penalty is logarithmic in nature yielding a large score for large differences close to 1 and small score for small differences tending to 0. <br>(see more : https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e)
<br><br><em>Exemple</em> : we consider a classification problem with 4 labels. After a few iterations we get a model precise enough to predict correctly the output. However the entropy is still important. After more iteration, the model gets more precise, and its entropy decreases.</div>
<center>
<img src="pictures/CE.png">
</center>
<center>
<img src="pictures/CE_opt.png">
</center></section>
</section>
<section id="regularization">
<h2>4. Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline">#</a></h2>
<p>The objectif of regularization if to avoid over-fitting. There are several ways to do so :</p>
<ul class="simple">
<li><p>Penalization</p></li>
<li><p>Soft weight sharing</p></li>
<li><p>Dropout</p></li>
<li><p>Batch normalization</p></li>
<li><p>Early stopping</p></li>
</ul>
<section id="penalization">
<h3>Penalization<a class="headerlink" href="#penalization" title="Permalink to this headline">#</a></h3>
<p>Penalization consists in adding a function <span class="math notranslate nohighlight">\( pen(\theta) \)</span> that will increases as the weigths <span class="math notranslate nohighlight">\( \theta \)</span> increases. It is therefore an optimisation problem defined as follow :</p>
<div class="math notranslate nohighlight">
\[
\min_{\theta}(\mathcal{L}(\theta,X,y) + \lambda pen(\theta))
\]</div>
<p>There are three penalisation functions :</p>
<ul class="simple">
<li><p>Lasso (or L1 norm):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
pen(\theta) = \lambda \vert\vert \theta \vert\vert_2^2
\]</div>
<ul class="simple">
<li><p>Ridge (or L2 norm):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
pen(\theta) = \lambda \vert\vert \theta \vert\vert_1
\]</div>
<ul class="simple">
<li><p>Elastic Net (combination of L1 and L2):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
pen(\theta) = \lambda \vert\vert \theta \vert\vert_2^2 + \lambda \vert\vert \theta \vert\vert_1
\]</div>
<p><em>Remider</em> : the p-norm is defined as :  <span class="math notranslate nohighlight">\( \left\|x\right\|_p = (\sum_{i=1}^{n}\vert x_i \vert^p)^{\frac{1}{p}} \)</span></p>
<p><em>Note</em> : the lasso regression induce sparsity, that is that the features that are not relevant are set to zero, what reduces the dimension of the weights vector</p>
</section>
<section id="dropout">
<h3>Dropout<a class="headerlink" href="#dropout" title="Permalink to this headline">#</a></h3>
<div style="text:justify">
Dropout consists in removing temporarily some neurons from the network in order to train it and test the cooadaptation of each neuron. 
It can been seen as an averaging method. Some researchers suggest to drop only the weights of the neurons.
We will see later in this chapter how to implement it in a neural network.
</div>
</section>
<section id="batch-normalization">
<h3>Batch normalization<a class="headerlink" href="#batch-normalization" title="Permalink to this headline">#</a></h3>
<p>Batch normalization aims at noramlizing the inputs in order the make the algorithm converge faster. In order to do so, we have to scale the inputs (mean, variance…).</p>
</section>
<section id="early-stopping">
<h3>Early stopping<a class="headerlink" href="#early-stopping" title="Permalink to this headline">#</a></h3>
<div style="text:justify">
The main idea behind early stopping is to stop the algorithm after k steps without improvement. We call the patience of 
the algorithm the number of times it will run without improvement before stopping.
</div></section>
</section>
<section id="in-conclusion">
<h2>5. In conclusion<a class="headerlink" href="#in-conclusion" title="Permalink to this headline">#</a></h2>
<div style="text-align:justify">
A neural network is composed of a succession of layers, each of them corresponding to a set of neurons. Each neuron will recieve an input and return an output value computed with an activation function. Therefore, the quality of the predictions of the neural network will depend on the choice of the activation function, the number of layers and the depth of each layer. Thus it is necessary to chose those hyperparameters with great caution. Then begin the training phase and the weights and biaises of each neuron will be computed through a recursive process. The training of the neural network implies choosing a regularization function, that will affect the values of the weights. 
<br><br>
Now, we will see how to use thoses notions in order to create a functionning neural network. 
</div></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./5. Deep Learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../4.%20Unsupervised%20ML/Classification.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Classification</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="2.%20Deep%20Learning%20Optimization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Deep Learning Problems and Optimization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Guillaume de Surville<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>